{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be02a78f-f9b1-4f14-be6e-2568e6d299e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "# from visualizer import get_local\n",
    "# get_local.activate()\n",
    "from playground import build_all_model\n",
    "import datasets.transforms as T\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import math\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from torch import nn\n",
    "torch.set_grad_enabled(False);\n",
    "\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
    "    parser.add_argument('--lr_backbone', default=1e-4, type=float)\n",
    "    parser.add_argument('--weight_decay', default=0.05, type=float)\n",
    "    parser.add_argument('--batch_size', default=2, type=int)\n",
    "    parser.add_argument('--lr_drop', default=200, type=int)\n",
    "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
    "                        help='gradient clipping max norm')\n",
    "\n",
    "    # Pix2Seq\n",
    "    parser.add_argument('--model', type=str, default=\"pix2seq\",\n",
    "                        help=\"specify the model from playground\")\n",
    "    parser.add_argument('--pix2seq_lr', action='store_true', help='use warmup linear drop lr')\n",
    "    parser.add_argument('--large_scale_jitter', action='store_true', help='large scale jitter')\n",
    "    parser.add_argument('--rand_target', action='store_true',\n",
    "                        help=\"randomly permute the sequence of input targets\")\n",
    "    parser.add_argument('--pred_eos', action='store_true', help='use eos token instead of predicting 100 objects')\n",
    "\n",
    "    # * Backbone\n",
    "    parser.add_argument('--backbone', default='swin_L', type=str,\n",
    "                        help=\"Name of the convolutional backbone to use\")\n",
    "    parser.add_argument('--dilation', action='store_true',\n",
    "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
    "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
    "                        help=\"Type of positional embedding to use on top of the image features\")\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument('--enc_layers', default=6, type=int,\n",
    "                        help=\"Number of encoding layers in the transformer\")\n",
    "    parser.add_argument('--dec_layers', default=6, type=int,\n",
    "                        help=\"Number of decoding layers in the transformer\")\n",
    "    parser.add_argument('--dim_feedforward', default=1024, type=int,\n",
    "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
    "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "    parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                        help=\"Dropout applied in the transformer\")\n",
    "    parser.add_argument('--nheads', default=8, type=int,\n",
    "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
    "    parser.add_argument('--pre_norm', action='store_true')\n",
    "\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
    "                        help=\"Relative classification weight of the no-object class\")\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--dataset_file', default='coco')\n",
    "    parser.add_argument('--coco_panoptic_path', type=str)\n",
    "    parser.add_argument('--device', default='cpu',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--resume', default='output/HRSC_4cls_v5/checkpoint_best.pth', help='resume from checkpoint')\n",
    "    parser.add_argument('--num_workers', default=2, type=int)\n",
    "    \n",
    "    parser.add_argument('--num_classes', default=4, type=int, help='max ID of the datasets')\n",
    "    \n",
    "    parser.add_argument('--img_path', default='./HRSC/Test/AllImages/100000647.jpg', type=str, help='the path to predict')\n",
    "    parser.add_argument('--swin_path', default='', help='resume from swin transformer')\n",
    "    parser.add_argument('--activation', default='relu', help='transformer activation function')\n",
    "    parser.add_argument('--input_size', default=1333, type=int, help='max ID of the datasets')\n",
    "    return parser\n",
    "\n",
    "class Colors:\n",
    "    # Ultralytics color palette https://ultralytics.com/\n",
    "    def __init__(self):\n",
    "        # hex = matplotlib.colors.TABLEAU_COLORS.values()\n",
    "        hex = ('FF3838', 'FF9D97', 'FF701F', 'FFB21D', 'CFD231', '48F90A', '92CC17', '3DDB86', '1A9334', '00D4BB',\n",
    "               '2C99A8', '00C2FF', '344593', '6473FF', '0018EC', '8438FF', '520085', 'CB38FF', 'FF95C8', 'FF37C7')\n",
    "        self.palette = [self.hex2rgb('#' + c) for c in hex]\n",
    "        self.n = len(self.palette)\n",
    "\n",
    "    def __call__(self, i, bgr=False):\n",
    "        c = self.palette[int(i) % self.n]\n",
    "        return (c[2], c[1], c[0]) if bgr else c\n",
    "\n",
    "    @staticmethod\n",
    "    def hex2rgb(h):  # rgb order (PIL)\n",
    "        return tuple(int(h[1 + i:1 + i + 2], 16) for i in (0, 2, 4))\n",
    "\n",
    "def plot_one_box(x, im, color=(128, 128, 128), label=None, line_thickness=3):\n",
    "    # Plots one bounding box on image 'im' using OpenCV\n",
    "    assert im.data.contiguous, 'Image not contiguous. Apply np.ascontiguousarray(im) to plot_on_box() input image.'\n",
    "    tl = line_thickness or round(0.002 * (im.shape[0] + im.shape[1]) / 2) + 1  # line/font thickness\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(im, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(im, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(im, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "        \n",
    "def plot_one_box_polyl(x, im, color=(128, 128, 128), label=None, line_thickness=3):\n",
    "    # Plots one bounding box on image 'im' using OpenCV\n",
    "    assert im.data.contiguous, 'Image not contiguous. Apply np.ascontiguousarray(im) to plot_on_box() input image.'\n",
    "    tl = line_thickness or round(0.002 * (im.shape[0] + im.shape[1]) / 2) + 1  # line/font thickness\n",
    "    points = np.array(x).reshape(4, 2).reshape(4, 1, 2).astype(int)\n",
    "    # c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    # cv2.rectangle(im, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    cv2.polylines(im, [points], True, color, line_thickness)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        # c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        # cv2.rectangle(im, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(im, label, (int(x[0]), int(x[1]) - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "        \n",
    "def plot_results(pil_img, prob, boxes, CLASSES, COLORS):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        cl = p.argmax()\n",
    "        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def PostProcess(args, output, names, h_ori, w_ori, h, w):\n",
    "    out_seq_logits = output['pred_seq_logits']\n",
    "    \n",
    "    orig_size = torch.as_tensor([int(h_ori), int(w_ori)])\n",
    "    size = torch.as_tensor([int(h), int(w)])\n",
    "    origin_img_sizes = torch.stack([orig_size], dim=0)\n",
    "    input_img_sizes = torch.stack([size], dim=0)\n",
    "    ori_img_h, ori_img_w = origin_img_sizes.unbind(1)\n",
    "    inp_img_h, inp_img_w = input_img_sizes.unbind(1)\n",
    "    num_bins = 2000\n",
    "    num_classes = args.num_classes\n",
    "    scale_fct = torch.stack(\n",
    "            [ori_img_w / inp_img_w, ori_img_h / inp_img_h,\n",
    "             ori_img_w / inp_img_w, ori_img_h / inp_img_h,\n",
    "             ori_img_w / inp_img_w, ori_img_h / inp_img_h,\n",
    "             ori_img_w / inp_img_w, ori_img_h / inp_img_h], dim=1).unsqueeze(1).to(args.device)\n",
    "    results = []\n",
    "    image = cv2.imread(args.img_path)\n",
    "    for b_i, pred_seq_logits in enumerate(out_seq_logits):\n",
    "        # print('pred_seq_logits'.format(pred_seq_logits))\n",
    "        seq_len = pred_seq_logits.shape[0]\n",
    "        if seq_len < 9:\n",
    "            results.append(dict())\n",
    "            continue\n",
    "        pred_seq_logits = pred_seq_logits.softmax(dim=-1)\n",
    "        num_objects = seq_len // 9\n",
    "        pred_seq_logits = pred_seq_logits[:int(num_objects * 9)].reshape(num_objects, 9, -1)\n",
    "        pred_boxes_logits = pred_seq_logits[:, :8, :num_bins + 1]\n",
    "        pred_class_logits = pred_seq_logits[:, 8, num_bins + 1: num_bins + 1 + num_classes]\n",
    "        # print(pred_class_logits)\n",
    "        scores_per_image, labels_per_image = torch.max(pred_class_logits, dim=1)\n",
    "        boxes_per_image = pred_boxes_logits.argmax(dim=2) * 1333 / num_bins\n",
    "        boxes_per_image = boxes_per_image * scale_fct[b_i]\n",
    "        result = dict()\n",
    "        result['scores'] = []\n",
    "        result['labels'] = []\n",
    "        result['boxes'] = []\n",
    "        idx = []\n",
    "        index = 0\n",
    "        for score, cls, box in zip(scores_per_image.detach().cpu().numpy(),\n",
    "                                         labels_per_image.detach().cpu().numpy(),\n",
    "                                         boxes_per_image.detach().cpu().numpy()):\n",
    "            box = box.tolist()\n",
    "            if score > 0.75:\n",
    "                result['scores'].append(score)\n",
    "                result['labels'].append(cls)\n",
    "                result['boxes'].append(box)\n",
    "                # print('box: ', box)\n",
    "                colors = Colors()\n",
    "                c = int(cls - 1)\n",
    "                # plot_one_box_polyl(box, image, label=names[c], color=colors(c, True), line_thickness=3)\n",
    "                idx.append(index)\n",
    "                index += 1\n",
    "            else:\n",
    "                index += 1\n",
    "                break\n",
    "        # cv2.imwrite('./result.jpg', image)\n",
    "        results.append(result)\n",
    "    print(results)\n",
    "    print(idx)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76853c-139e-4b94-a485-3e3eec472b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser('Pix2Seq training and evaluation script', parents=[get_args_parser()])\n",
    "args = parser.parse_args(args=['--device', 'cpu'])\n",
    "\n",
    "names = ['QHJ', 'XYJ', 'DLJ', 'YSJ', 'LGJ', 'HKMJ', 'ZHJ', 'QT', 'HC', 'KC', 'BZJ', 'YLC', 'ship']\n",
    "names = ['ship', 'aircraft carrier', 'warcraft', 'merchant ship']\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "device = torch.device(args.device)\n",
    "\n",
    "model, _, _ = build_all_model[args.model](args)\n",
    "\n",
    "checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'], strict=False)\n",
    "\n",
    "model.to(device)\n",
    "#read image\n",
    "image = Image.open(args.img_path)\n",
    "image = image.convert('RGB')\n",
    "\n",
    "w_ori, h_ori = image.size\n",
    "print(image.size)\n",
    "# image = np.array(image).astype(np.uint8)\n",
    "\n",
    "#transform\n",
    "normalize = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform = T.Compose([\n",
    "    # T.RandomResize([800], max_size=1333),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "image_new = transform(image, None)\n",
    "\n",
    "c,h,w = image_new[0].shape\n",
    "print(c, h, w)\n",
    "image_new = image_new[0].view(1, c, h, w).to(device)\n",
    "seq = torch.ones(1, 1).to(device,dtype=torch.long) * 2001\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4fd94b4e-d558-4099-a128-5d264e9225ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use lists to store the outputs via up-values\n",
    "swin_output, conv_features, enc_attn_weights, dec_self_weights, dec_attn_weights =[], [], [], [], []\n",
    "mean = [0.485,0.456,0.406]\n",
    "std = [0.229,0.224,0.225]\n",
    "hooks = [\n",
    "    model.backbone[0].body.norm3.register_forward_hook(\n",
    "        lambda self, input, output: swin_output.append(output)\n",
    "    ), # (1, h*w, 1536)\n",
    "    model.backbone[0].input_proj3[1].register_forward_hook(\n",
    "        lambda self, input, output: conv_features.append(output)\n",
    "    ), # (1, 256, h, w)\n",
    "    model.transformer.encoder.layers[-1].ms_deformbale_attn.register_forward_hook(\n",
    "        lambda self, input, output: enc_attn_weights.append(output[1])\n",
    "    ), # attn: (1, h, w, 8, 4) offset: (1, h, w, 8, 8)\n",
    "    model.transformer.decoder.layers[-1].self_attn.register_forward_hook(\n",
    "        lambda self, input, output: dec_self_weights.append(output)\n",
    "    ), # list(([1, 1, 256], [1, 1, h*w]), .....)\n",
    "    model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(\n",
    "        lambda self, input, output: dec_attn_weights.append(output)\n",
    "    ), # list(([1, 1, 256], [1, 1, h*w]), .....)\n",
    "]\n",
    "\n",
    "# propagate through the model\n",
    "output = model([image_new, seq])\n",
    "\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# don't need the list anymore\n",
    "swin_output = swin_output[0]\n",
    "# cv2.imwrite('swin_output.jpg', (conv_features[0][0, 0, ...] * 255).reshape(26, 38, 1).numpy())\n",
    "conv_features = conv_features[0]\n",
    "enc_attn_weights = enc_attn_weights[0]\n",
    "dec_attn_weights = dec_attn_weights\n",
    "\n",
    "print(swin_output.shape)\n",
    "print(enc_attn_weights['attns'].shape)\n",
    "print(len(dec_attn_weights))\n",
    "print(dec_attn_weights[0][0].shape)\n",
    "print(dec_attn_weights[0][1].shape)\n",
    "_, attn_h , attn_w, _, _ = enc_attn_weights['attns'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "25713ca5-2485-4905-ab0c-10443154716e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'scores': [], 'labels': [], 'boxes': []}]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "results = PostProcess(args, output, names, h_ori, w_ori, h, w)\n",
    "num_boxes = len(results[0]['scores'])\n",
    "colors = Colors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674954a-a104-47a6-9829-06076c509daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "fig, axs = plt.subplots(ncols=num_boxes, nrows=2, figsize=(100, 45))\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "image_source = cv2.imread(args.img_path)\n",
    "for idx, ax_i in zip(range(num_boxes), axs.T):\n",
    "    ax = ax_i[0]\n",
    "    image_copy = image_source.copy()\n",
    "    box = results[0]['boxes'][idx]\n",
    "    c = results[0]['labels'][idx]\n",
    "    ax.imshow(image_copy)\n",
    "    x = box[::2]\n",
    "    y = box[1::2]\n",
    "    points = list(zip(x,y))\n",
    "    polygon= plt.Polygon(points,  fill=False, edgecolor='r', linewidth=3)\n",
    "    ax.add_patch(polygon)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Class id: {c}', fontsize=200)\n",
    "    dec_attn_weights_object = torch.zeros(attn_h, attn_w)\n",
    "    for i in range(8):\n",
    "        # ax = ax_i[i + 1]\n",
    "        # ax.imshow(dec_attn_weights[idx*9 + i][1].reshape(26, 34))\n",
    "        dec_attn_weights_object = dec_attn_weights[idx*9 + i][1].reshape(attn_h, attn_w) + dec_attn_weights_object\n",
    "        # ax.axis('off')\n",
    "    ax = ax_i[1]\n",
    "    ax.imshow(dec_attn_weights_object)\n",
    "    ax.axis('off')\n",
    "fig.tight_layout()\n",
    "plt.savefig('visualize.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a514395-fcdc-4b7a-979e-bb7554f83ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
